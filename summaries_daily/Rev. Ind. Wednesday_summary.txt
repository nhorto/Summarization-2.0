Tekla PowerFab Consulting — Topic‑Organized Session Summary
(All statements below are drawn directly from the session transcript. No assumptions or external information have been added.)

1) Database, Server & IT Infrastructure
- What was discussed / current state
  - A new Oracle database had been provisioned and was to be set up during the session.
  - The client planned a complete redo of servers (“the whole strip of service”). The new server room will be on‑premise in the same area as the current main office (“the sugar room on premise”) and was described as “a whole lot more beefy.”
  - The original ITA/ITA key and “the original IT guy” (Keith) were mentioned as part of past integration and IT history. Keith had previously assembled the “Frankenstein” stack and had sent e‑mail about SP1 patch dates; the latest release noticed was v2035 (SP2).
  - The client had a copy of a “blank database” and discussed restoring/cleaning the instance (disable sizes, clear what’s not needed). IT availability and restart coordination were noted as constraints; an IT call was implied necessary.
  - A quick test to access port 307 was proposed; the transcript recorded attempts and a new port that was “not” communicating.
- Key concerns / risks
  - Server rebuild and migration were high‑impact items; IT responsiveness was flagged as a risk (“IT is probably restarting the server… IT is slow”).
  - Patch and setting persistence: custom settings are repeatedly destroyed on updates and must be re‑applied.
- Decisions / changes made during session
  - Re‑instatement of maintenance coverage had been completed (maintenance had expired on 1/30/25 and was reinstated with a set price for 2025).
- Explicit next steps / action items
  - Configure the new Oracle database and integrate with PowerFab (action item recorded).
  - Plan and schedule the server infrastructure overhaul; obtain the required key and move server‑based reports to the server (“I’ll get the key… put all of that on the server”).
  - Coordinate an IT restart/port connectivity test (port 307 and the new port) and restore/verify the clean database instance; coordinate with IT for the restore and pruning.

2) Vista Integration, Labor Export & Excel Mapping
- What was discussed / current state
  - All labor data was exported to Vista via Excel. The export included employee number (example value referenced: “39,000”), hours worked, and a phase code (e.g., “third”).
  - Export sheet processing: the Excel sheet was filtered, cascaded, and “read‑mapped” into Vista; Vista then pulled the 39,000 employee number and associated values.
  - The client described a two‑system flow: a face‑recognition system captures identity and in/out times, and Tekla receives/holds production time; the face system was used to verify identity and then Tekla exports to Vista via the Excel method.
  - TechLog2W was explicitly referenced for the export to Excel: “we submit the actual Excel sheet from this, from TechLog2W.”
  - The client considered the Vista integration as intended to remove manual Excel manipulation, but observed it still required manual filtering and mapping (“creates double work”).
- Key details preserved from transcript
  - Employee number example: 39,000.
  - Export format: Excel (filtered, cascaded, read‑mapped into Vista).
  - Export source: TechLog2W (explicit).
- Concerns / limitations
  - The Excel export/import step remained a manual/costly step; management noted “double work” and questioned ROI of the current integration.
  - Some data access/sharing questions were raised (“they wouldn’t want to give me some”).
- Decisions / changes made
  - No automation beyond existing Excel export was implemented during the session.
- Explicit next steps / action items
  - Validate the Excel labor‑export process and mapping into Vista (confirm employee numbers, hours, phase codes are correctly mapped).
  - Evaluate feasibility of linking facial‑recognition data directly into Tekla to reduce manual handling (explore API options—task recorded).
  - Clarify what reporting/data can be shared between systems and who owns the mapping logic.

3) Time‑Capture, Face‑Recognition & Time‑Tracking Practice
- What was discussed / current state
  - Primary time‑capture method: face‑recognition entry at the gate/monitor. The system captured employee identity and timestamps and saved “in a year‑tracking and production” file that was verified against face records.
  - Workflow included mapping face entries to project/workstation and splitting time across up to “five persons” per record if needed.
  - The face system’s outputs were reconciled against Tekla production exports before creating the Excel sheet for Vista.
  - Manual entry time frustrations were described: entering time in Tekla/EPM took “7 8 9 9 9 9 9 10 minutes” per entry for some users; an existing workaround was to have a paper sheet passed to “Nolan” who would enter all data (~5 minutes per entry).
- Reporting tied to time capture
  - Daily labor recap email included prior day hours, negative trend alerts, and percent complete metrics; weekly production‑schedule sheet was reviewed each Friday.
- Concerns / limitations
  - User interface (“a little clunky”) and Notes field clutter were flagged; desire for more fast/touch panel UI was expressed.
  - The current process still required filtering and reformatting prior to export; errors could occur if wrong values were entered (example: “36 instead of 3 or 6”).
- Decisions / changes made
  - No technical changes were made to the face system during session; the client reiterated desire to automate the Excel re‑format step and reduce manual corrections.
- Explicit next steps / action items
  - Assess the feasibility of direct ingestion of face‑recognition data into Tekla to remove manual Excel steps.
  - Define requirements for a custom export that contains only necessary columns and reduces human reformatting (consider Crystal Reports).
  - Map and document the exact daily/weekly recap content and recipients (e.g., distribution to “Sarah” was discussed).

4) Production Planning, Work Packages & Production Planner Usage
- What was discussed / current state
  - Work packages had been added to the Production Planner but the Production Binder was not yet used at “full‑scale.”
  - The Production Planner was described as the most efficient tool for release work and forecasting; the client had not fully used it yet for large‑scale scheduling.
  - Release workflow: the consultant and Alan outlined a process for releasing work in PowerFab and scheduling work package releases; Microsoft Project schedules were maintained “on the side” to forecast across platforms.
  - Work packages were to be shipped/staged directly to site per area rather than “shake everything out.”
- Specific operational elements
  - One‑week work packages were discussed, with a “one week ahead” / “one week coming back” rotation for carts and assemblies.
  - Cart quantity uncertainty: client asked if jobs used “three carts” or “fifty carts”; consultant noted expectation of “one e to 30 in rotation” (i.e., up to 30).
  - For remote shops (Iowa, Texas), each location was described as having its own fabshop and rotation; New Mexico was identified as “the farthest one we’re gonna have to figure out.”
- Concerns / limitations
  - Production Planner had not been run as a long pilot yet; training and process documentation were required.
- Decisions / changes made
  - Agreed to begin a pilot use of Production Planner and to schedule release workflow testing between consultant and Alan.
- Explicit next steps / action items
  - Begin a pilot of the Production Planner to test release work and forecasting.
  - Verify production routes, route setup, and codes used in the routes (“white topics” review).
  - Establish carts and shipping plan for New Mexico; clarify cart rotation counts and labeling rules.

5) Estimating, Labor Rates, Percentages & Adjustment Workflow
- What was discussed / current state
  - The client had low confidence in current labor estimations; they reported the estimator’s rate limit was “vastly different” from expectations.
  - Estimator default behavior: when a “start rebellion” (new estimate) is submitted, the default completion percentage is 98%; the estimator can then adjust it up or down.
  - The client referenced a target of 75% (multiple contexts): the system or internal target had historically used 75% and the client wanted to align settings to a 75% target in some places.
  - Efficiency math: example that if a job is at 90% efficiency the hours calculation divides by 0.9 to inflate hours. The practice of dividing by 0.9 was explicitly cited (“If it’s at 90% efficiency … the calculation of the hours divides by .9”).
  - A recommended approach discussed later: enter the original estimate into the “Original Estimate” column (e.g., 30 h) and remove values from the Adjustment column so percent release and base hours reconcile correctly.
- Specific numeric examples preserved
  - Default estimator completion percentage: 98% (on new estimate submission).
  - Client target for tweaking: 75% (referenced repeatedly).
  - Efficiency divisor example: divide hours by 0.9 for 90% efficiency.
  - Example original estimate/base hours: 30 h original, 16 h base; client wanted original estimate put into the base‑hours field to compute correct percent release and contingency (14 h).
- Concerns / limitations
  - Multiple jobs showed high variance (examples cited in transcript: 40% weight difference; various burn % values across jobs). The client saw “no pattern whatsoever” in several burn/tech/fit‑up comparisons.
  - New estimators were unfamiliar with system usage; some estimators performed takeoffs outside Tekla and imported results manually.
- Decisions / changes made during session
  - The client agreed to change practice: put the original estimate into the base/original estimate field and clear the adjustment column; this was explicitly discussed and agreed (“That makes sense. Yeah, we’ll make that change.”)
- Explicit next steps / action items
  - Review and adjust estimator settings to align default completion percentages and rate limits to client expectations (investigate the 75% vs 98% references).
  - Enter original estimate values into the base/original estimate column and remove the adjustment entries; run tests to confirm percent release behaves as expected.
  - Provide estimator training on the estimator workflow, cycling feature, and base vs adjustment handling.

6) Labor Export, Reporting & Daily/Weekly Recap
- What was discussed / current state
  - Daily labor recap emails were in use, showing hours used the previous day, negative trend alerts, and percent complete; weekly production schedule sheets were reviewed each Friday.
  - A “PAM report” was mentioned as a potential consolidated view across projects but it was not confirmed whether it existed.
  - The client receives a summary of labor entered the previous day and reviews percent complete and hours by sequence.
- Concerns / limitations
  - The export workflow required manual filtering, remapping and clean‑up before Vista import, producing duplicate manual work.
- Explicit next steps / action items
  - Confirm availability of a consolidated daily/weekly labor report (PAM or equivalent) and arrange distribution to stakeholders (e.g., Sarah).
  - Document the daily/weekly recap fields and ensure automated generation where possible.

7) Wire Nesting, Material Specification & Nesting Library
- What was discussed / current state
  - The client requested the addition of wire sizes and gauges to the nesting library; wire dimensions should be kept in the same linear metric as existing round‑bar definitions.
  - Wire purchase quantity example: Stephen and Michael could purchase up to 3,000 ft of wire for “schools” (phrase from transcript).
  - Single conduit runs were currently being treated as “BON” (non‑nesting), leading to a “still not nesting” state — essentially a shipable output without nesting optimization.
  - The client reported using chat‑generated nesting for wire with utilization dropping (~15% drop reflected; transcript recorded “utilization came down to like… 15% drop” and “0.04” in a separate mention).
- Concerns / limitations
  - Curved cuts on wire were not present in current workflows; some “little trick” had been developed for wire handling (“working so far”).
- Explicit next steps / action items
  - Add wire sizes/gauges to the nesting library, ensuring metric consistency with round‑bar. (Action item identified.)
  - Review wire nesting utilization and consider rollout of the “wire trick” more broadly if validated.

8) Cutting, Burn Rates, Plasma, CNC & Welding Throughput
- What was discussed / current state
  - Multiple cut technologies were discussed: plate plasma cutting, plasma shape cutting, softening, and torch/manual burn; the client said “plasma shape cutting… still sucked” for shapes.
  - Specific machine/performance numbers cited explicitly:
    - Machine’s weld capability ≈ 20 inches per minute for a two‑pass weld (applied up to half‑inch thickness).
    - Another reference: machine can do 22 in/min (welding throughput).
    - Estimating guideline used: 5 ft / hour baseline for a three‑day production window.
    - Example feed rates: “10 inches per minute for quarter H.A.K.” (equated with 550 ft/hr in one place – transcript: “10 inches per minute for quarter H.A.K. (equivalent to 550 ft/hr)”).
    - A possible processor speed setting recorded as “6‑3.”
    - Several references to “5.5” and “69” (contextual values mentioned verbatim in transcript).
  - Pass count discussion for welding:
    - Quarter‑inch typically single pass.
    - Half‑inch typically two passes (“it’s a two pass well for us… up to half‑inch”).
    - 3/8‑inch likely requires more than one pass.
  - Burn/perimeter length formulas and length formula edits were demonstrated (delete “Burn Man” entry, create new burn operation; revise length/perimeter formula to use parentheses and multiply by constants—e.g., “3 × 2 × (perimeter)”) — these maintenance steps were shown in the session.
  - Start/stop time capture was described as “the awesome way to do it” for station timing, but a warning was given to watch schedule effects and double‑counting.
- Concerns / limitations
  - Piercing/time to move head/turn arcs and non‑cut head movement were not included in some automated speeds; the consultant cautioned that published machine numbers might be overly optimistic.
  - ACPC (Azure ACPC time) integration had color codes and failure modes; if ACPC goes down the system “just goes past it” which was a concern.
  - Variation between cutting machines: one machine on estimates was “twice as fast” as another; currently not tracked in the estimate.
- Explicit next steps / action items
  - Pull runtime reports from Land Tech / “land tech” to capture machine runtimes and validate burn rates.
  - Edit operation formulas and create new burn entries where needed (demonstrated; to be applied).
  - Run spot start/stop time studies for stations to capture real start/stop behavior and avoid double‑counting.

9) Coatings, Finish Families & Service‑Prep
- What was discussed / current state
  - Three coating families were configured/added: high‑solids, steel‑spark (steel spec), and Acropoxy (transcript also used “macro epoxy” in a later section).
  - Container sizes were configured to match purchasable units; fields captured: cost per gallon and average cost (co‑average) for estimate calculations.
  - The system now produced gallons required per job, total material cost, and summary including square‑foot inputs.
  - Finish families entered included B2, SP3, SP6 and a “simple 1‑2‑6” family; additional complex paint routes (prep → intermediate → prime → finish) were created and can be marked up in EP and exported for inspection.
  - The coating description did not carry through a “travel interval” (explicit).
- Concerns / limitations
  - The client warned that cost per gallon must be validated before bid submission; the system used co‑average that needs to be current.
  - Difficulty obtaining “right specs” to populate the finish board was expressed.
- Explicit next steps / action items
  - Verify cost‑per‑gallon values before final bids.
  - Validate the paint route and traveler outputs on the next live job; confirm inspection linkage.
  - Add finish spec options to estimates and confirm how they appear on travelers.

10) Assemblies, Revit/IFC/KISS/Excel Workflows & P1000 / R1 / R2
- What was discussed / current state
  - Revit families were being used to design unit components; Revit → IFC exchange frequently stripped metadata (“I lose all of my metadata… exported model is just a body shape”).
  - The client planned to create a stencil spreadsheet (Excel) containing the required parameter fields (quantity, drawing number, description, grade, length) to import into PowerFab. The Excel/KISS import path was the preferred approach.
  - P1000 was repeatedly referenced as the primary assembly/part number (many statements: “P1000… a few one thousand… the actual P1000 number”).
  - R1 and R2 components were discussed: each must keep consistent labeling; they cannot be interchanged without consistent labeling, though the transcript noted they “could technically change those.”
  - TR1 was described as a “single‑tiered, trippy support.”
  - Thousands of trapeze supports and LED 400 were cited as components worth modeling.
- Concerns / limitations
  - IFC export from Revit removed needed metadata; the client intended to rely on Excel/KISS as the reliable import mechanism.
  - Revit families were described as “messy” and “hard to handle” in current workflow; a Revit plug‑in or EcoStructure family approach was discussed as a potential avenue.
- Explicit next steps / action items
  - Produce an Excel export template (version one) that includes required fields and the additional column for rod piece‑marking; client offered to send screenshots for column layout.
  - Create a VOM in Revit and demonstrate the import table fields that will be used; then use Excel/KISS import to PowerFab.
  - Finalize P1000 & R1/R2 naming convention: keep assembly‑specific labels and implement piece‑mark suffix convention (e.g., R1 + last three digits) per transcript discussion.

11) Splices, Labor Kits & Material Breakdowns
- What was discussed / current state
  - Column splices were present on a labor kit and users could “use this splice,” but the transcript recorded “most people don’t use it—they don’t put any material in themselves, and they don’t put labor to it.”
  - The current view “doesn’t give you a breakdown of what it is,” which the client wanted cleaned up.
  - Splice operations were tracked as 12 hours when splicing pieces together; splice time covered both shop and field prep; the splice entry had to be selected explicitly.
- Explicit next steps / action items
  - Review and clean up event breakdowns for column splices and labor kit entries.
  - Ensure that splices display a clear material and labor breakdown in the model/report.

12) Quality Control (QC), Inspection & QC Bucket Usage
- What was discussed / current state
  - Each project had a separate, empty bucket added for QC purposes. QC dollars were tracked on a completely separate code for each project (QC code example referenced as 011400).
  - QC person still provided a thumbs‑up inspection report; however, responsibility had shifted so “the quality guy is not responsible anymore; it’s just the bidder that’s responsible.”
  - QC sampling approach described: first‑article inspection and periodic sampling (every 10% sampling was referenced as a concept).
  - The client asked whether marking an assembly complete could be treated as a QC inspection and whether inspection records could be filled within the system; the system allowed marking completion even if inspection had not been performed.
- Explicit next steps / action items
  - Formalize QC sampling plan (first‑article + periodic sample, e.g., every 10%).
  - Configure inspection reports to be linked to service‑prep and assemblies and confirm how the inspection data travels with the job.

13) Material Handling, Packaging, Carts & Shipping Logistics
- What was discussed / current state
  - Packaging approach: a “card” concept that contained all required parts/traps and was labelled for easier movement; the client wanted packaging to be “easy to move.”
  - Carts: question whether jobs used 3 carts or 50; consultant suggested a rotation up to “one e to 30 in rotation” (i.e., up to 30 carts).
  - Pre‑fab parts were described as “the first set of parts you used to ship out of the job scene”; pre‑fab activity was a major early step.
  - For multi‑location coordination: Iowa and Texas have their own fabshops and rotation; New Mexico required a shipping plan and cost recovery.
  - Asset tracking: field crews were removing assets and they “disappear”; a token‑based return tracking concept was discussed to capture receipt dates and prompt returns.
- Explicit next steps / action items
  - Define cart labeling and rotation policy (R1 + last three digits label convention was discussed).
  - Create a shipping & cost‑recovery plan for New Mexico.
  - Implement an asset checkout/return token workflow to track carts and assemblies deployed to the field.

14) Facilities, Capacity & Site Planning
- What was discussed / current state
  - A new facility plan was discussed: 65,000 sq ft of space with “one bay for tanks” and “one bay for spatial work”; “seven of our guys” would be assigned.
  - Data‑center skids were mentioned alongside a figure of 40 billion (explicit quoted phrase: “data‑center skids were mentioned with a figure of 40 billion”).
  - The client reported a $700,000 spend on “selling our beams” (statement quoted verbatim).
  - Multiple facilities were discussed (Denver, Great Falls, Casper, Gilbert, Phoenix, etc.). Great Falls had overhead cranes but little work; Casper and other locations were to be analyzed for capabilities.
- Concerns / limitations
  - Many facilities lacked full structural capability; some sites were “not really set up to do structural.”
- Explicit next steps / action items
  - Continue collecting capability data for each facility (equipment, personnel, throughput).
  - Confirm footprint and resource planning for proposed 65,000 sq ft facility.

15) Licensing, Module Access & Multi‑Database Considerations
- What was discussed / current state
  - The client requested “all of their modules” and discussed licensing constraints (“some of them are not giving you access because the license process is too low”).
  - Statement: “the system can point to only one database at a time” (single database pointer per site was explicitly stated).
  - For separate experimental environments or different databases, a separate server instance/IP would be required.
- Explicit next steps / action items
  - Confirm licensing allocations and which modules were to be provisioned for users.
  - Assess the feasibility of a separate server instance for testing/isolated database work.

16) Reporting, Exports & Archive Management
- What was discussed / current state
  - The client valued visibility to import logs and asked to place reports on the server to avoid being overridden by updates; the consultant agreed to do so and to obtain a key for server‑based deployment (“I’ll get the key”).
  - A historical weekly report process previously performed by “Fred Dutton” had lapsed after Fred stopped working there.
  - The client wanted an ability to “dump it all in one swoop into Excel” because “budget numbers don’t match well” and to run estimate vs. actual comparisons for a set of jobs.
- Explicit next steps / action items
  - Move key reports to the server and secure the key to control the report files.
  - Produce an Excel export of schedule and BOM data (version one) for client manipulation and provide mapping columns.
  - Document archive locations (Mark’s folder, R: / J: drive references) and share links to procedures.

17) Training, User Adoption & Support Website
- What was discussed / current state
  - Training gaps identified: new estimators were unfamiliar with Tekla features; some staff “don’t even know how to use” the estimator tools. Ricky offered to lead or facilitate training.
  - The consultant planned to produce a support website with chat boards and monthly “web hours” where clients can discuss ACPC and other topics; the site would host community discussions and support.
- Explicit next steps / action items
  - Provide estimation training exercises for new estimators (set up a short training session).
  - Launch the support website and schedule the first monthly web hour; provide access and announce the support community.

18) Estimating Data Quality, Time Studies & Global Factors
- What was discussed / current state
  - The need for larger, validated samples for time studies was emphasised; for example, a single sample across different job sizes (40 h vs 4,000 h) was insufficient.
  - Discussion of global factor alignment: add/adjust range bands (e.g., “add another 20 band”) and maintain global factors to benefit all projects.
  - Start/stop station time‑studies were promoted as an “awesome way to do it” with appropriate safeguards against double counting.
- Explicit next steps / action items
  - Perform structured time studies (grouped by rule) and build the rule set into a central maintenance file.
  - Implement spot checks and validation across multiple job sizes to converge on global factors.

19) Miscellaneous Explicit Items Recorded (verbatim phrases)
- Server / port tests (port 307), Oracle DB setup, SP1/SP2 notes (v2035), “start rebellion” default completion 98%, 75% target, employee number 39,000, wire purchase up to 3,000 ft, 30,000 ft referenced in one place, $700,000 spend on “selling our beams”, 65,000 sq ft new facility, 40 billion figure associated with data‑center skids, 5 ft/hour baseline, machine capacity ≈ 20 in/min or 22 in/min, 550 ft/hr equivalence for a quoted 10 in/min earlier, 96 × 240 table size, 40‑ft plates discussed, splice time = 12 hours, QC code example 011400, indirect labor job 1200, kit/PH final estimate practice, P1000 and R1/R2 references.

20) Consolidated Action Items & Owners (as explicitly stated in transcript)
- Configure the new Oracle database and integrate with the PowerFab environment. (Client / IT / Consultant)
- Add wire sizes/gauges to the nesting library; keep metric consistent with round‑bar definitions. (Client / Consultant)
- Begin pilot use of the Production Planner to test release‑work workflow (Consultant + Alan / Production team).
- Review and adjust estimator settings (default completion percentage, rate limits) to align with the client’s target (75% where appropriate). (Estimating lead / Consultant)
- Validate the Excel labor‑export process and mapping into Vista (confirm employee numbers, hours, phase codes). (Client / Consultant)
- Confirm availability of consolidated daily/weekly labor report (PAM or equivalent) and arrange distribution to stakeholders (e.g., Sarah). (Client / Reporting)
- Plan and schedule the server infrastructure overhaul and restore/clean database instance; obtain server key and move reports to server. (IT / Consultant)
- Prepare and deliver a version‑one Excel export (schedule/BOM) for client manipulation; add requested columns (rod piece‑mark suffix, etc.). (Consultant)
- Pull runtime reports from Land Tech and examine machine runtimes and burn data (Client / Production / Consultant).
- Perform start/stop station time studies (spot checks) to validate cut/handling times and prevent double counting. (Shop supervisor / Consultant)
- Review and clean up event breakdowns for splices and labor kit entries; ensure splice material & labor breakdowns are visible. (Estimating / Production)
- Formalize QC sampling and link inspection reporting to assemblies and service‑prep flows (Quality lead / Consultant).
- Implement asset checkout/return token workflow for carts/field assets and define cart rotation policy (Operations / Logistics).
- Provide estimator training (cycling feature, FBA, base vs adjustment practice) and document procedures on shared drive (Training coordinator / Consultant).
- Review coating cost per gallon values before final bids; confirm finished traveler output and inspection linkage (Estimating / Coatings lead).

21) Closing observations recorded in transcript
- The session covered a broad set of topics; the recurring themes were: tighten documentation and procedures, align estimator defaults with production reality, clean up labor/category structures, automate manual Excel rework where possible, and rebuild server/database infrastructure to be more robust.
- The transcript contained many conversational and personal remarks; the items summarized above are exclusively the explicit, project‑relevant statements from the transcript.

If you want, I will:
- Produce a prioritized, time‑phased workplan (with estimated owners and due dates) based solely on the action items above, or
- Produce a concise checklist of the minimal server/database and Vista‑export steps to be implemented first.

Which would you prefer as the next deliverable?